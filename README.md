## Development
To get the dependencies (Redis and pip installables), simply run `make deps`. Because dddb currently doesn't seem to work in virtual environments, do this _outside of a venv_.

To start everything, simply run `make` (or `make all`). This will handle daemonization for you.

In the event you need to kill supervisord, run `make kill`.

## The prototype message format
Derived from P3, all messages are expected to be valid, flat (for now) JSON documents with the following structure:
```json
{
    "msg_id": "123e4567-e89b-12d3-a456-426614174000",
    "message_type": "command_request",
    "initiated_by": "server",
    "timestamp": 1702103078.145753,
    "data": "Hello world!!!!!"
}
```

`data` may or may not be base64 encoded for the sake of this agent. If `data` is a valid base64 string, it is decoded to bytes. Else, the message is assumed to be in plaintext. The messages generated by this agent are *always* base64 encoded.

## Project notes
[Celery Intro](https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html#first-steps)

To set up RabbitMQ:
 - `sudo apt-get install rabbitmq-server`
 - `sudo rabbitmq-server` (if needed)

One-time:
- `sudo rabbitmqctl add_user myuser mypassword`
- `sudo rabbitmqctl add_vhost myvhost`
- `sudo rabbitmqctl set_user_tags myuser mytag`
- `sudo rabbitmqctl set_permissions -p myvhost myuser ".*" ".*" ".*"`

Then define tasks in `tasks.py`. The backend is where results get stored; this can be the Django ORM, Redis, or (since we've used RabbitMQ here), RPC.

To start the worker server, run
`celery -A tasks worker --loglevel=INFO`

at which point you can define stuff that imports and runs stuff off of `tasks.py` as desired.

Note that `celery beat` is a different thing from `celery worker`; `beat` is responsible for issuing periodic tasks to `worker`, which is responsible for actually executing the tasks. To start `beat`, run

`celery -A tasks beat`


For redis, just install redis normally and `pip install -U celery[redis]`, then run `redis-server`. So:
- Run `redis-server`
- `celery -A tasks beat --loglevel=INFO`
- `celery -A tasks worker --loglevel=INFO`
- (venv) `python3 src/main.py`


So to start and daemonize everything, just run `supervisord` in the root; by default, it will look for `supervisord.conf` in the current directory before looking elsewhere (`/etc/supervisord.conf`). You'll still need to run the app itself.
